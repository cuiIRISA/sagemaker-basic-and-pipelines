{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore, Develop, Train, Optimize and Deploy Built-in algorithm XGBoost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we show how to use Amazon SageMaker to develop, train, tune and deploy a built-in XGBoost model. We continue to use the Boston Housing dataset, present in Scikit-Learn: https://scikit-learn.org/stable/datasets/index.html#boston-dataset\n",
    "\n",
    "**This sample is provided for demonstration purposes, make sure to conduct appropriate testing if derivating this code for your own use-cases!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tarfile\n",
    "\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "\n",
    "\n",
    "sm_boto3 = boto3.client('sagemaker')\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "bucket = sess.default_bucket()  # this could also be a hard-coded bucket name\n",
    "\n",
    "print('Using bucket ' + bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites: prepare the raw dataset\n",
    "#### We load a dataset from sklearn library, split it and send it to S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_boston\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = load_boston()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.data, data.target, test_size=0.25, random_state=42)\n",
    "\n",
    "trainX = pd.DataFrame(X_train, columns=data.feature_names)\n",
    "trainX['target'] = y_train\n",
    "\n",
    "testX = pd.DataFrame(X_test, columns=data.feature_names)\n",
    "testX['target'] = y_test\n",
    "\n",
    "trainX.to_csv('boston_train.csv')\n",
    "testX.to_csv('boston_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload the dataset to S3 as input data for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send data to S3. SageMaker will take training data from s3\n",
    "trainpath = sess.upload_data(\n",
    "    path='boston_train.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/xgboostcontainer/raw-data')\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path='boston_test.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/xgboostcontainer/raw-data')\n",
    "\n",
    "print('Raw dataset will be stored S3 at:', trainpath)\n",
    "print('Raw dataset will be stored S3 at:', testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon SageMaker Experiments – Organize, Track And Compare Your Machine Learning Trainings\n",
    "\n",
    "* Doc https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html\n",
    "* SDK https://sagemaker-experiments.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install sagemaker-experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Experiments automatically tracks the inputs, parameters, configurations, and results of your iterations as trials. You can assign, group, and organize these trials into experiments. SageMaker Experiments is integrated with Amazon SageMaker Studio providing a visual interface to browse your active and past experiments, compare trials on key performance metrics, and identify the best performing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from time import strftime\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "from smexperiments.trial import Trial\n",
    "from smexperiments.trial_component import TrialComponent\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "experiment_name = \"Boston-Housing-XGBoost-Trial-{}\".format(create_date)\n",
    "demo_experiment = Experiment.create(experiment_name = experiment_name,\n",
    "                                    description = \"Demo experiment using SageMaker for organize, track and compare\"\n",
    "                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A trial is a set of steps called trial components that produce a machine learning model. A trial is part of a single Amazon SageMaker experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "demo_trial = Trial.create(trial_name = \"Boston-Housing-XGBoost-Trial-{}\".format(create_date),\n",
    "                          experiment_name = experiment_name\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a trial component, which is a stage of a machine learning trial. A trial is composed of one or more trial components. Trial components include pre-processing jobs, training jobs, and batch transform jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Tracker.create(display_name=\"Dataset\", sagemaker_boto_client=sm_boto3) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        \"train-test-splite\": 70\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name=\"boston-housing-training-dataset\", media_type=\"s3/uri\", value=trainpath)\n",
    "    tracker.log_input(name=\"boston-housing-test-dataset\", media_type=\"s3/uri\", value=testpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_trial_component = tracker.trial_component\n",
    "demo_trial.add_trial_component(dataset_trial_component)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing with Amazon SageMaker Processing\n",
    "Amazon SageMaker Processing allows you to run steps for data pre- or post-processing, feature engineering, data validation, or model evaluation workloads on Amazon SageMaker.\n",
    "\n",
    "* Doc https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html\n",
    "* SDK https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write preprocessing script with scikit-learn\n",
    "\n",
    "This simple script preprocesses data into SageMaker Built-in XGBoost compatible format, by changing the colume order of training and test dataset, and by dropping the header of dataset and some columns. In real world cases, you can image a more complete pre-processing setup with Amazon SageMaker Processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "columns = ['CRIM', 'ZN', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'target']\n",
    "\n",
    "if __name__=='__main__':\n",
    "    \n",
    "    sagemaker_processing_input_path = '/opt/ml/processing/input'\n",
    "    sagemaker_processing_output_path = '/opt/ml/processing/output'\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-file', type=str, default='boston_train.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='boston_test.csv')\n",
    "    parser.add_argument('--input-dir', type=str, default=sagemaker_processing_input_path)\n",
    "    parser.add_argument('--output-dir', type=str, default=sagemaker_processing_output_path)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    print('Received arguments {}'.format(args))\n",
    "\n",
    "    print('reading data')\n",
    "    train_df = pd.read_csv(os.path.join(args.input_dir, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.input_dir, args.test_file))\n",
    "        \n",
    "    cols_xgboost = columns[-1:] + columns[:-1]\n",
    "    \n",
    "    train_df = train_df[cols_xgboost]\n",
    "    test_df = test_df[cols_xgboost]\n",
    "    \n",
    "    # Create local output directories\n",
    "    if not os.path.exists(os.path.join(args.output_dir,'train')):\n",
    "        os.makedirs(os.path.join(args.output_dir,'train'))\n",
    "        print('creating the processed train directory')\n",
    "\n",
    "    if not os.path.exists(os.path.join(args.output_dir,'test')):\n",
    "        os.makedirs(os.path.join(args.output_dir,'test'))\n",
    "        print('creating the processed test directory')\n",
    "    \n",
    "    output_train_data_path = os.path.join(args.output_dir,'train',args.train_file)\n",
    "    train_df.to_csv(output_train_data_path,header=False,index=False)\n",
    "    print('Saved the processed training dataset')\n",
    "\n",
    "    \n",
    "    output_test_data_path = os.path.join(args.output_dir,'test',args.test_file)\n",
    "    test_df.to_csv(output_test_data_path,header=False,index=False)\n",
    "    print('Saved the processed test dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the code locally on this local notebook environment\n",
    "\n",
    "This code runs on the Docker image associated with this notebook, you can run with command line outside the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python preprocessing.py  --input-dir './' \\\n",
    "                           --output-dir './processed' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process data with Amazon SageMaker Processing Job\n",
    "\n",
    "You can run a scikit-learn script to do data processing on SageMaker.\n",
    "The code runs a processing job using SKLearnProcessor class from the the Amazon SageMaker Python SDK to execute a scikit-learn script that you provide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "role = get_execution_role()\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the input and output S3 location for SageMaker Processing Job with SKLearnProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_s3 = 's3://{}/sagemaker/xgboostcontainer/raw-data'.format(bucket)\n",
    "print('Raw dataset at S3 location:',input_data_s3)\n",
    "output_data_s3_prefix = 's3://{}/sagemaker/xgboostcontainer/processed'.format(bucket)\n",
    "print('Processed dataset at S3 location:',output_data_s3_prefix)\n",
    "output_data_s3_train = output_data_s3_prefix + '/train'\n",
    "output_data_s3_test = output_data_s3_prefix + '/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run SageMaker Processing Job with SageMaker SDK\n",
    "\n",
    "See the SDK reference\n",
    "https://sagemaker.readthedocs.io/en/stable/amazon_sagemaker_processing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "current_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_data_s3,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[ProcessingOutput(output_name='xgboost_train_data',\n",
    "                                                source='/opt/ml/processing/output/train',\n",
    "                                               destination = output_data_s3_train),\n",
    "                               ProcessingOutput(output_name='xgboost_test_data',\n",
    "                                                source='/opt/ml/processing/output/test',\n",
    "                                               destination = output_data_s3_test)],\n",
    "                      experiment_config={ \"TrialName\": demo_trial.trial_name, \"TrialComponentDisplayName\": \"Preprocessing-{}\".format(current_date)}\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Training with built-in XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker provides several built-in machine learning algorithms that you can use for a variety of problem types.\n",
    "<br>\n",
    "Using the built-in algorithm version of XGBoost is simpler than using the open source version, because you don’t have to write a training script. \n",
    "\n",
    "* Doc https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html\n",
    "* SDK https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve \n",
    "from sagemaker.session import Session\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "# specify the repo_version depending on your preference.\n",
    "container = retrieve(region=boto3.Session().region_name,\n",
    "                          framework='xgboost', \n",
    "                          version='1.0-1')\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the hyperparameters for SageMaker Built-in XGBoost.\n",
    "<br>\n",
    "In terms of objective metric, we fix here reg:squarederror, which indicates regression task with squared loss. \n",
    "\n",
    "List of available hyperparameters can be found here \n",
    "<br>\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost_hyperparameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize hyperparameters\n",
    "hyperparameters = {\n",
    "        \"max_depth\":\"10\",\n",
    "        \"eta\":\"0.2\",\n",
    "        \"gamma\":\"4\",\n",
    "        \"min_child_weight\":\"6\",\n",
    "        \"subsample\":\"0.7\",\n",
    "        \"objective\":\"reg:squarederror\",\n",
    "        \"num_round\":\"200\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launching a training job with the Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a SageMaker estimator that calls the xgboost-container\n",
    "estimator = sagemaker.estimator.Estimator(image_uri=container, \n",
    "                                          hyperparameters=hyperparameters,\n",
    "                                          role=role,\n",
    "                                          instance_count=1, \n",
    "                                          instance_type='ml.m5.2xlarge')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the data type and paths to the training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/sagemaker/xgboostcontainer/processed/{}/\".format(bucket, 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/sagemaker/xgboostcontainer/processed/{}/\".format(bucket, 'test'), content_type=content_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the XGBoost training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "estimator.fit({'train': train_input, 'validation': validation_input},       \n",
    "              experiment_config={\n",
    "                \"TrialName\": demo_trial.trial_name,\n",
    "                \"TrialComponentDisplayName\": \"Training-{}\".format(current_date)},\n",
    "              wait=False\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy an endpoint for real-time inference with SageMaker SDK \n",
    "\n",
    "Here we deploy the best trained job to an Amazon SageMaker endpoint with SageMaker SDK \n",
    "<br>\n",
    "Note that one could also use the more extensive process of creating a model from s3 artifacts, and deploy a model that was trained in a different session or even out of SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m5.xlarge', endpoint_name ='xgboost-endpoint',\n",
    "                             tags=None, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invoke with boto3 python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "prediction_data = np.array([0.09178,0.0,6.416,84.1,2.6463,5.0,296.0,16.6,395.5,9.04]).reshape((1,10))\n",
    "serialized_data = pd.DataFrame(prediction_data).to_csv(header=False, index=False).encode('utf-8')\n",
    "print(serialized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv serialization\n",
    "response = runtime.invoke_endpoint(\n",
    "    EndpointName=predictor.endpoint_name,\n",
    "    Body=serialized_data,\n",
    "    ContentType='text/csv')\n",
    "\n",
    "print(response['Body'].read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch prediction with batch transform\n",
    "\n",
    "Run inference when you don't need a persistent endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestedpath = sess.upload_data(\n",
    "    path='./processed/test/boston_test.csv', bucket=bucket,\n",
    "    key_prefix='sagemaker/xgboostcontainer/ingested-data')\n",
    "\n",
    "print('Ingested data will be stored S3 at:', ingestedpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The location of the test dataset\n",
    "batch_input = 's3://{}/sagemaker/xgboostcontainer/ingested-data/'.format(bucket)\n",
    "\n",
    "# The location to store the results of the batch transform job\n",
    "batch_output = 's3://{}/sagemaker/xgboostcontainer/batch-predicted-data/'.format(bucket)\n",
    "\n",
    "\n",
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = estimator.transformer(instance_count=1, instance_type='ml.m5.xlarge',\n",
    "                                            output_path=batch_output,accept='text/csv',assemble_with='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line', input_filter='$[:29]')\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line', \n",
    "                      input_filter=\"$[1:]\", join_source= \"Input\", output_filter=\"$\")\n",
    "\n",
    "print('Waiting for transform job: ' + transformer.latest_transform_job.job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker Hyperparameters Tuning with Built-in XGBoost\n",
    "\n",
    "Check out the SageMaker documentation for How Hyperparameter Tuning Works\n",
    "<br>\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html\n",
    "\n",
    "Similar as the SageMaker training job SDK, we configure here the SageMaker estimator, and pre-set the hyperparameters when we consider fixed (no need to tune).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    instance_count=1, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    sagemaker_session=sess)\n",
    "\n",
    "xgb.set_hyperparameters(objective='reg:squarederror',\n",
    "                        num_round=50,\n",
    "                        rate_drop=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an objective metric and a set of the hyperparameters to be tuned, the tuning job optimizes a model for the metric that you choose.\n",
    "\n",
    "<br>\n",
    "For regression problem, we fix here Root Mean Square Error (RMSE) as objective metric for tuning job, and the best job would be the one minimises such error.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'validation:rmse'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform automatic model tuning with following hyperparameters\n",
    "\n",
    "- eta: Step size shrinkage used in updates to prevent overfitting. After each boosting step, you can directly get the weights of new features. The eta parameter actually shrinks the feature weights to make the boosting process more conservative.\n",
    "- alpha: L1 regularization term on weights. Increasing this value makes models more conservative.\n",
    "- min_child_weight: Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, the building process gives up further partitioning. In linear regression models, this simply corresponds to a minimum number of instances needed in each node. The larger the algorithm, the more conservative it is.\n",
    "- max_depth: Maximum depth of a tree. Increasing this value makes the model more complex and likely to be overfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n",
    "                        'min_child_weight': ContinuousParameter(1, 10),\n",
    "                        'alpha': ContinuousParameter(0, 2),\n",
    "                        'max_depth': IntegerParameter(1, 10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launch the SageMaker hyperparameter tuning job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(xgb,\n",
    "                            objective_metric_name=objective_metric_name,\n",
    "                            objective_type=objective_type,\n",
    "                            hyperparameter_ranges=hyperparameter_ranges,\n",
    "                            max_jobs=4,\n",
    "                            max_parallel_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "content_type = \"csv\"\n",
    "train_input = TrainingInput(\"s3://{}/sagemaker/xgboostcontainer/processed/{}/\".format(bucket, 'train'), content_type=content_type)\n",
    "validation_input = TrainingInput(\"s3://{}/sagemaker/xgboostcontainer/processed/{}/\".format(bucket, 'test'), content_type=content_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.fit({'train': train_input, 'validation': validation_input},\n",
    "          include_cls_metadata=False,wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetch results about a hyperparameter tuning job and make them accessible for analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tuner results in a df\n",
    "results = tuner.analytics().dataframe()\n",
    "results.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end, please don't forget to delete the endpoint !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3.delete_endpoint(EndpointName=tuning_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
